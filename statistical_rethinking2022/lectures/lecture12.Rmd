# Lecture 12: Multi Level Models

Revisiting the trolly problem

\begin{tikzcd}
&& U \arrow[dl] \\
X \arrow[r] & R & S \arrow[l] \\
E \arrow[ur] & A \arrow[l] \arrow[u] & G \arrow[bend left=30,ll] \arrow[ul]
\end{tikzcd}

now with individual $U$. So how does one add memory to this model?

## Partial Pooling

For intance in our case the individual might have their own preference, simply replace the fixed $\sigma$ in a prior to a fit prior. (exp(1) is a good prior for such a distribution). The fit $\sigma$ then represents the memory in observations.


### Note

- Fitting for $\sigma$ adds dependecies for your other priors, reducing flexibility, in other words the effective parameters.
- Adding new (correctly identified causal) variables to the model will also reduce the fit $\sigma$.
  - By adding treatments one by one we can observe things about the size of effects we see, remember the highly non linear effect of parameters in our GLMs.

### The Three Great Superstitions

- Different levels **do not** need to be sampled at random
- **Do not** need large sample sizes
- This **does not** assume Gaussian variation

## Mundlak Machines

Suppose one had a model:

\begin{tikzcd}
& U \arrow[d] \arrow[dl] \\
X \arrow[r] & Y \\
& Z \arrow[u]
\end{tikzcd}

with unobserved $U$ and group level trait $Z$. How does one dela with $US$.

### Fixed Effect Model

$$\begin{aligned}
Y_{i}        &= \mathrm{Bernoulli}(p_{i}) \\
logit(p_{i}) &= \alpha_{U[i]} + \beta_{X} X_{i} + \beta_{Z} Z_{U[i]} \\
\alpha_{j}   &= \mathrm{Normal}(0,10) \\
\beta_{j}    &= \mathrm{Normal}(0,1).
\end{aligned}$$

Inefficient, soaking up group level (fixed) effects of the confound $US, but cannot identify $Z$ (Technically we shouldn't include $Z$ at all in the model).

### Partial Pooling

$$\begin{aligned}
Y_{i}        &= \mathrm{Bernoulli}(p_{i}) \\
logit(p_{i}) &= \alpha_{U[i]} + \beta_{X} X_{i} + \beta_{Z} Z_{U[i]} \\
\alpha_{j}   &= \mathrm{Normal}(\phi,\tau) \\
\beta_{j}    &= \mathrm{Normal}(0,1) \\
\phi         &= \mathrm{Normal}(0,1) \\
\tau         &= \mathrm{Exponential}(1).
\end{aligned}$$

Better estimate for $U$, worse for $X$. However we can now measure $U$.

### Mundlak Machine

\begin{tikzcd}
X' & U \arrow[l] \arrow[d] \arrow[dl] \\
X \arrow[r] & Y \\
& Z \arrow[u]
\end{tikzcd}

with group average $X'$ and modified partial pooling logit:

$$logit(p_{i}) = \alpha_{U[i]} + \beta_{X} X_{i} + \beta_{Z} Z_{U[i]} + \beta_{X'} X'_{U[i]}.$$

Better $X$ but ignore quality of $X'$ for each group.

### Latent Mundlak Machine (Mundlak FLB)

$$\begin{aligned}
Y_{i}        &= \mathrm{Bernoulli}(p_{i}) \\
logit(p_{i}) &= \alpha_{U[i]} + \beta_{X} X_{i} + \beta_{Z} Z_{U[i]} + \beta{GY} G[U] \\
X_{i}        &= \mathrm{Normal}(\mu_{i}, \sigma) \\
\mu_{i}      &= \alpha_{X} + \beta_{GX} G[U] \\
\end{aligned}$$

The best in all regards except computational time.
