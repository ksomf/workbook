# Lecture 2: Introdution to Bayesian Inference

```{r FigureBlueDot, echo=FALSE, include=TRUE, out.width='0.49\\linewidth', fig.align='center', fig.cap=c('(ref:FigureBlueDotCaption)')}
knitr::include_graphics('external_images/globe_east_2048.jpg')
```

(ref:FigureBlueDotCaption) The famous NASA blue dot from Visible Earth [@visibleEarth] which we can think of throwing imaginary asteroids at to sample.

How can we measure what percentage of the earth is covered with water; Spherical uniform sampling (Figure \@ref(fig:FigureBlueDot)). But how do we quantify uncertainty in our measurement? We use Bayesian data analysis.

In essence it is all just counting. The advantage of this is that we can update our most likely conjecture by taking another measurement (Baysian updating).

 1. State a causal model for observations arrise, given each possible explanation
 2. Count the ways the data could arrive for each explanation
 3. Relative plausibility is relative value from (2)

## Garden of Forking Data

```{r garden_code}
generate_garden <- function(bag,levels,picks=NULL){
	len <- length(bag)
	x   <- seq(0.5,len-0.5,length.out=len)
	if( is.null(picks) ){
		picks = rep(FALSE,levels)
	}
	
	if( levels == 1 ){
		picked=bag==picks[1]
		return(list(points=tibble(values=bag, x=x, y=1, picked=picked  %>% as.integer()), lines=tibble()))
	}else{
		pick      <- picks[1]
		next_pick <- picks[2]
		picked      <- bag == pick
		next_picked <- bag == next_pick
		children_garden <- generate_garden(bag,levels-1,tail(picks,-1))
		furthest_row <- children_garden$points %>% subset(y == max(y))
		closest_row  <- children_garden$points %>% subset(y == min(y))
		
		x <- x * nrow(furthest_row)
		new_points <- tibble(values=bag, x=x, y=1, picked=picked)
		points_branch <- children_garden$points %>% mutate(y=y+1)
		old_points <- tibble( values = rep(points_branch$values,times=len)
						   	, x      = outer(points_branch$x,x,'+') %>% as.vector() - min(x)
						   	, y      = rep(points_branch$y,times=len)
							, picked = outer(points_branch$picked,picked,'*') %>% as.vector())
		nearest_points <- old_points %>% subset(y == min(y))
		points <- bind_rows(new_points,old_points)
		
		if( levels > 2 ){
			lines_branch <- children_garden$lines %>% mutate(y_start=y_start+1,y_end=y_end+1)
			old_points_in_branch <- old_points %>% subset(y > min(y))
			old_lines <- tibble( y_start = old_points_in_branch$y - 1
			                   , y_end   = old_points_in_branch$y
							   , x_start = outer(lines_branch$x_start,x,'+')      %>% as.vector() - min(x)
							   , x_end   = outer(lines_branch$x_end  ,x,'+')      %>% as.vector() - min(x)
							   , picked  = outer(lines_branch$picked ,picked,'*') %>% as.vector() %>% as.integer())
		}else{
			old_lines <- tibble()
		}
		new_lines <- tibble( y_start=1, y_end=2
					  	   , x_start=rep(x,times=rep(len,len))
						   , x_end  =nearest_points$x
						   , picked =outer(next_picked,picked,'*') %>% as.vector() %>% as.integer())
		lines <- bind_rows(new_lines,old_lines)
		return(list(points=points,lines=lines))
	}
}

combine_gardens <- function(gardens,sep=2){
	points <- gardens %>% lapply(function(x) x$points) %>% bind_rows(.id='chunk')
	lines  <- gardens %>% lapply(function(x) x$lines ) %>% bind_rows(.id='chunk')
	gaps <- points %>% 
		group_by(chunk) %>% 
		subset(y==max(y)) %>% 
		summarise(n=n()) %>% 
		mutate(n=lag(n+sep,n=1,default=sep/2)) %>% #c(sep/2,n+sep) %>% head(-1)) %>% 
		mutate(ac=cumsum(n)) %>% 
		select(-n)
	points <- left_join(points,gaps,by='chunk') %>% mutate(x=x+ac)
	lines  <- left_join(lines ,gaps,by='chunk') %>% mutate(x_start=x_start+ac,x_end=x_end+ac)
	vlines <- gaps %>% mutate(ac = ac - sep/2)  %>% pull(ac)
	return(list(points=points,lines=lines,vlines=vlines))
}

draw_garden <- function(points,lines,pick_alpha=TRUE,vertical_lines=NULL){
	colours <- points$values %>% unique()
	values2colour <- ggthemes::tableau_color_pal()(colours %>% length()) %>% setNames(colours)
	leaves <- points %>% subset(y == max(y))
	levels <- points %>% pull(y) %>% max()
	#garden$points <- garden$points %>% mutate(fill=values2colour[values] %>% as.vector())

	if( !pick_alpha ){
		points$picked <- 1
		lines$picked  <- 1
	}
	
	p <- points %>% 
		ggplot(aes(x=x,y=y)) +
		geom_point(aes(fill=values,alpha=picked),shape=21, size=3) +
		geom_segment(aes(x=x_start,xend=x_end,y=y_start,yend=y_end,alpha=picked),data=lines) +
		coord_polar() +
		scale_x_continuous(limits=c(0,leaves$x %>% max %>% ceiling + 1) ,breaks=NULL) +
		scale_y_continuous(limits=c(1/levels,levels+1),breaks=NULL) +
		scale_fill_manual(values=colours) +
		theme( legend.position = "none"
			  , panel.grid = element_blank()
			  , axis.title = element_blank()
			  , panel.background = element_rect(fill = "transparent",colour = NA)
			  , plot.background = element_rect(fill = "transparent",colour = NA) )
	if( !is.null(vertical_lines) ){
		p <- p + geom_vline(xintercept = vertical_lines, colour='black' )
	}
	return(p)
}
```

We start in the simpler case of finite possibilities. Suppose we are picking (with replacement) marbles out of a bag, if we have four marbles; one green, three red, we can determine all possibilities of getting three marbles (Figure \@ref(fig:MarbleGardenFigure)).

```{r MarbleGardenFigure, echo=FALSE, include=TRUE, out.width='0.49\\linewidth', fig.align='center', fig.cap=c('(ref:MarbleGardenFigureCaption)'), fig.subcap=c('Universe of possible picks','Universe after picks'), fig.asp=1, fig.ncol=2 }
bag = c('green','red','red','red')
picks = c('red','green','red')
garden <- generate_garden(bag,length(picks),picks)
draw_garden(garden$points,garden$lines,pick_alpha=FALSE)
draw_garden(garden$points,garden$lines,pick_alpha=TRUE)
```
	
(ref:MarbleGardenFigureCaption) The branching universe of possibilities of picking `r length(picks)` marbles with replacement in `r bag`, as well as the possibilities for picking `r picks`.

Now instead suppose we wanted to find out the proportion of marbles in the bag without prior knowledge, instead we examine the relative likelihood of different possibilities of combinations within the bag (Figure \@ref(fig:MarbleVerseFigure)).

```{r MarbleVerseFigure, echo=FALSE, include=TRUE, out.width='0.9\\linewidth', fig.align='center', fig.cap=c('(ref:MarbleVerseFigureCaption)')}
bag_size = 4
picks = c('red','green','red')
gen_universe = function(bag_green){
	if(bag_green == 0){
		bag = 'green'
	}else if(bag_green == bag_size){
		bag = 'red'
	}else if(all(c(bag_size,bag_green) %% 2 == 0)){
		bag = rep(c('green','red'),times=c(bag_green/2,(bag_size-bag_green)/2))
	}else{
		bag = rep(c('green','red'),times=c(bag_green,bag_size-bag_green))
	}
	return(generate_garden(bag,length(picks),picks))
}
garden = map(seq(0,bag_size),gen_universe) %>% combine_gardens()
draw_garden(garden$points,garden$lines,pick_alpha=TRUE,vertical_lines=garden$vlines)
```
(ref:MarbleVerseFigureCaption) The universes of possible two colour four marble equivalent bags.

## Posterior: Bayes' Theorem

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

Can think of it as updating our prior knowledge of the answer $P(A)$ with new measurement $P(B|A)$, giving the new prior $P(A|B)$ given the new data $B$.

For instance in our water case the distribution based on uniform samples is just
$$ (1-x)^nx^m $$
for $n$ instances of land and $m$ instances of water. This distribution is a binomial distribution (Figure \@ref(fig:BinomFigure)).
$$ P(W,L|p) = \frac{(W+L)!}{W!L!}p^W(1-p)^L $$
or
$$ P(W in N|p) = \frac{N!}{W!(N-W)!}p^W(1-p)^{N-W}. $$
So Bayes' gives
$$ P(p|W,L) = \frac{P(W,L|p)P(p)}{P(W,L)} $$
making the updating step much easier to understand. Note that a flat prior, ie each $p$ between $0$ and $1$ is equally likely; a flat prior.



So the next question how does one report a result from such a distribution. Can one use the mean or median; well generally no the distribution is the answer, and these ---point estimates--- remove some of the complexity of your data, of course publications may want you to present such an arbitrary value. Well then maybe a confidence interval to communicate some of the shape; again no, but somewhat more useful, it is just a reduction of the distribution. For instance a 50% interval doesn't really describe the data that well as the center or first or last 50% are equally valid. Something like a 99% interval does have more of a use describing how well something is described, but it is really just arbitrary -- **The distribution is the answer** -- 95% intervals don't have anything to do with robustness.

```{r BinomFigure, echo=TRUE, include=TRUE, out.width='0.49\\linewidth', fig.align='center', fig.cap=c('(ref:BinomFigureCaption)')}
ps = seq(0,1,length.out=1000)
prior = rep_along(ps,1) #flat prior
labels = list('1'='1/1','2'='3/6','3'='5/8')
d = data.frame( ps=ps
              , ys1 = dbinom(1,1,ps)*prior
              , ys2 = dbinom(3,6,ps)*prior
              , ys3 = dbinom(5,8,ps)*prior ) %>%
	mutate( ys1 = ys1/sum(ys1), ys2=ys2/sum(ys2), ys3=ys3/sum(ys3) ) %>%
	pivot_longer( ys1:ys3, names_to='dist', names_prefix='ys', values_to='ys')
ggplot(d) + 
	geom_line(aes(x=ps,y=ys,colour=dist), size=2) +
	scale_colour_manual(name='Samples Water', labels=c('1/1','3/6','5/8'), values=ggthemes::tableau_color_pal()(3))
```

(ref:BinomFigureCaption) Binomial distributions for proportion of true given specified true and total measurements.

## From Posterior to Prediction

To make actual predictions, we model of the posterior distribution against some question. The simple way is to take samples of the posterior and create a predictive distribution given the sampled $p$ for a number of discrete samples $n$. This predictive distribution is then sampled to construct the posterior predictive distribution through accumulation. These two steps are just rbinom in R, see (Figure \@ref(fig:PosteriorPredictiveFigure)), completed also for the prior, showing the results expected by a repeat experiment.

```{r PosteriorPredictiveFigure, echo=TRUE, out.width='0.49\\linewidth', fig.align='center', fig.cap=c('(ref:PosteriorPredictiveFigureCaption)') }
n_samples=1e4
w = 6
size = 9

ps = seq(0,1,length.out=n_samples)
prior = rep_along(ps,1)
probability = dbinom( w, size=size, prob=ps )
posterior = probability * prior
posterior = posterior / sum(posterior)

samples = sample( ps, prob=posterior, size=n_samples, replace=TRUE )
posterior_predictive = rbinom( n_samples, size=size, prob=samples )

samples_prior = sample( ps, prob=prior/sum(prior), size=n_samples, replace=TRUE )
prior_predictive = rbinom( n_samples, size=size, prob=samples_prior )

d = data.frame( x   =c(posterior_predictive,prior_predictive)
			  , type=rep( c('Posterior Predictive','Prior Predictive')
			  	 	    , times=c(n_samples,n_samples)))

d %>% 
	ggplot(aes(x=x,colour=type,fill=type)) + 
	geom_histogram(binwidth=1,position='dodge') + 
	labs(title=paste0(w,' wins in ',size,' samples posterior predictive'))
```

# Week 1 Homework

1. Suppose the globe toss data had been 4 water, 11 land. Construct the posterior distribution (using grid approximation) with flat prior.

```{r week01_q01, echo=TRUE}
water = 4
land = 11

n_samples = 1e4

ps = seq(0,1,length.out=n_samples)
prior = rep_along(ps,1)
probability = dbinom(water,water+land,ps)
posterior = probability * prior
posterior = posterior / sum(posterior)

d = data.frame( x=ps, y=posterior )
ggplot(d) + 
	geom_line(aes(x=x,y=y), colour=ggthemes::tableau_color_pal()(1), size=2)
```

2. The same but 4 water, 2 land and step prior at $p=0.5$

```{r week01_q02, echo=TRUE}
water = 4
land = 2

n_samples = 1e4

ps = seq(0,1,length.out=n_samples)
prior = rep(c(0,1),times=c(n_samples/2,n_samples/2))
probability = dbinom(water,water+land,ps)
posterior = probability * prior
posterior = posterior / sum(posterior)

d = data.frame( x=ps, y=posterior )
ggplot(d) + 
	geom_line(aes(x=x,y=y), colour=ggthemes::tableau_color_pal()(1), size=2)
```

3. Compute (assumed central) 89% percentile and HPDI intervals from (2).

```{r week01_q03, echo=TRUE}
samples = sample( ps, prob=posterior, size=n_samples, replace=TRUE )
c(HPDI(samples),PI(samples))
```
