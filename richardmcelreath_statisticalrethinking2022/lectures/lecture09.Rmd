# Lecture 9: Modelling Events

The learning example is influence of admission rates, with per department data. This leads to a very common basic mediating pathway:

\begin{tikzcd}
& D \arrow[dr] \\
G \arrow[ur] \arrow[rr] && A
\end{tikzcd}

Remember again the data itself does not contain any of the causes, so in these discrimination based research is difficult and requires carefull work.

## Types of Discrimination

In the literature we divide direct discrimination

- **Status Based (statistical) Discrimination:** Not direct against knowledge of the category being discriminated
- **Taste Based Discrimination:** Direct causal effect from category

Then the mediating paths are indirect discriminations; **structual discrimination**. In our example even if each department has equal admission rates on gender, overall there could still be total discrimination.

## GLM

Switching from a linear model to generalised linear models we go from

$$ \begin{aligned}
Y_i   &\sim Normal(\mu_i,\sigma) \\
\mu_i &= \alpha + \beta_X X_i + \beta_Y Y_i
\end{aligned} $$

to

$$ \begin{aligned}
Y_i   &\sim Bernoulli(p_i,\sigma) \\
f(p_i) &= \alpha + \beta_X X_i + \beta_Y Y_i
\end{aligned} $$

with some function $f$; the link function. So we can use this to restrict the probability to $[0,1]$. Then

$$ p_i = f^{-1}(\alpha + \beta_X X_i + \beta_Y Y_i) $$

### Logit Link

Arrissing naturally from normal distributions, the logit function is way of mapping $[0,1]$ to $\mathbb{R}$ without distortion. The logit function is just the log odds

$$ logit(p_i) = \log \frac{p_i}{1-p_i} $$

which has the logistics function as inverse

$$ logit^{-1}(q_i) = \frac{e^{q_i}}{1+e^{q_i}} $$

In practice this works really well, which is the real reason we use it. It is then fairely easy to read log odd values as really $logit(6) \approx 1$ and $logit(0) = 0.5$.

```{r logit}
x_bound <- 6
df   <- data.frame( x=seq(-x_bound,x_bound,length.out=1e6) )
df$y <- inv_logit(df$x)

ggplot(df) + geom_line(aes(x=x,y=y),colour=ggthemes::tableau_color_pal()(1))
```

Now the question arises as to good priors starting from the simplest case, constant value.

```{r LogitPrior}
samples <- 1e6
sigmas <- c(10,1.5,1)
df <- map_dfr(set_names(sigmas,sigmas), function(s) data.frame(x=rnorm(samples,sd=s)), .id='sigma') %>% 
	mutate(inv = inv_logit(x))

ggplot(df,aes(x=x)) +
	geom_density(colour=ggthemes::tableau_color_pal()(1)) +
	facet_grid(~sigma) +
	xlim(-25,25)

ggplot(df,aes(x=inv)) +
	geom_density(colour=ggthemes::tableau_color_pal()(1)) +
	facet_grid(~sigma)
```
So a reasonable flat distribution is $\sigma = 1.5$ and a distribution that emphasizes non extreme values $\sigma = 1$. Of course the large sigma strongly favours extreme results.

### Stan matrix notation

```{r StanMatrixNotation, eval=FALSE}
alist(
	A ~ bernoulli(p),
	logit(p) <- a[G,D],
	matrix[G,D]:a ~ normal(0,1)
)
```

### Binomial Regression

Depending on data structure the equiavlent binmial regression to

$$ \begin{aligned}
A_i        &\sim Bernoulli(p_i) \\
logit(p_i) &= \alpha[G_i,D_i]
\end{aligned} $$

is

$$ \begin{aligned}
A_i        &\sim Binomial(N_i,p_i) \\
logit(p_i) &= \alpha[G_i,D_i]
\end{aligned} $$

moving $[0,1] \to [0 ... N]$.

### Marginal Causal Effect

Now beware, when we perform an intervention on gender, we are really changing the percieved gender $G \to P \to A$. This can have subtle implications on what the question we are actually answering.

### Beware

Discrimination effects can hide in all sorts of places, for instance from the department choice itself; here be confounds.

# Week 4 Homework

## Q1: Marriage Age and Happiness

\begin{tikzcd}
& A \arrow[dr] \arrow[dl] \\
M \arrow[rr] && H
\end{tikzcd}

```{r MarriageDataLoad}
library(cmdstanr)
df <- sim_happiness() %>% 
	subset(age >= 18) %>% 
	mutate( M = married + 1
		  , A = (age-18)/(max(age)-18)
		  , H = happiness )
```

```{r MarriageModel1} 
a6.9 <- alist( H ~ dnorm(mu,sigma)
	         , mu <- a[M] + bA * A
	         , a[M]  ~ dnorm(0,1)
	         , bA    ~ dnorm(0,2)
	         , sigma ~ dexp(1) )
m6.9m <- ulam(a6.9, data=df, chains=4, cores=2)
m6.9q <- quap(a6.9, data=df)
precis(m6.9m,depth=2)
traceplot_ulam(m6.9m)
trankplot(m6.9m)
```

```{r MarriageModel2}
a6.10 <- alist( H ~ dnorm(mu,sigma)
		      , mu <- a + bA * A
		      , a     ~ dnorm(0,1)
		      , bA    ~ dnorm(0,2)
		      , sigma ~ dexp(1) )
m6.10m <- ulam(a6.10, data=df, chains=4, cores=2)
m6.10q <- quap(a6.10, data=df)
precis(m6.10m,depth=2)
traceplot_ulam(m6.10m)
trankplot(m6.10m)
```
```{r MarriageModelComparison}
compare(m6.9q, m6.10q, func=WAIC)
compare(m6.9q, m6.10q, func=PSIS)
```

Answer WAIC and PSIS both say that the first model is better, but it is clear from the DAG that stratification on age opens a non causal path through age.

## Q2: Urban Foxes Revisited

```{r FoxesMk2}
data(foxes)

foxes <- foxes %>% 
	mutate(across(-any_of('group'),standardize)) %>% 
	rename(F=avgfood, G=groupsize, A=area, W=weight)

ftow_full <- quap(
	alist(
		W~ dnorm( mu, sigma ),
		mu <- a + bF * F,
		a ~ dnorm(0,0.3),
		bF ~ dnorm(0,0.6),
		sigma ~ dexp(1)
	), 
	data=foxes
)
ftow_direct <- quap(
	alist(
		W~ dnorm( mu, sigma ),
		mu <- a + bF * F + bG * G,
		a ~ dnorm(0,0.3),
		bF ~ dnorm(0,0.6),
		bG ~ dnorm(0,0.6),
		sigma ~ dexp(1)
	), 
	data=foxes
)

compare(ftow_full, ftow_direct, func=WAIC)
compare(ftow_full, ftow_direct, func=PSIS)
```

There is no real difference between the two, but $a$ represents the normal weight for the wolf, and $\beta_F$ the total change in weight from a hypothetical intervention on food.

## Q3: Cherry Blossom Precition


\begin{tikzcd}
Y \arrow[r] & T \arrow[r] & D
\end{tikzcd}

```{r CherryBlossomWrangling}
data(cherry_blossoms)
df_raw <- cherry_blossoms %>% 
	select(year,doy,temp) %>% 
	subset(complete.cases(.)) %>% 
	rename(Y=year, T=temp, D=doy)
df <- mutate(df_raw, across(-any_of('Y'),standardize))
	

a_const <- alist( D ~ dnorm(mu, sigma)
                , mu <- a
				, a     ~ dnorm(0,1)
				, sigma ~ dexp(1) )
a_linear <- alist(  D ~ dnorm(mu, sigma)
                 , mu <- a + bT * T
			     , a     ~ dnorm(0,1)
				 , bT    ~ dnorm(0,1)
				 , sigma ~ dexp(1) )
a_quadratic <- alist(  D ~ dnorm(mu, sigma)
                    , mu <- a + bT * T + bT2 * T**2
			        , a     ~ dnorm(0,1)
		     	    , bT    ~ dnorm(0,1)
					, bT2   ~ dnorm(0,1)
				    , sigma ~ dexp(1) )
a_cubic <- alist(  D ~ dnorm(mu, sigma)
                , mu <- a + bT * T + bT2 * T**2 + bT3 * T**3
			    , a     ~ dnorm(0,1)
		     	, bT    ~ dnorm(0,1)
				, bT2   ~ dnorm(0,1)
				, bT3   ~ dnorm(0,1)
				, sigma ~ dexp(1) )

m_const     <- quap(a_const    , data=df)
m_linear    <- quap(a_linear   , data=df)
m_quadratic <- quap(a_quadratic, data=df)
m_cubic     <- quap(a_cubic    , data=df)

compare(m_const, m_linear, m_quadratic, m_cubic, func=PSIS)
```

```{r CherryBlossomPrediction}
temperature <- 9
z <- (temperature - mean(df_raw$T))/sd(df_raw$T)

res <- data.frame(doy_p=sim( m_linear, data=list(T=z))) %>% 
	mutate(D = doy_p * sd(df_raw$D) + mean(df_raw$D))

ggplot(res,aes(x=D)) +
	geom_density(colour=ggthemes::tableau_color_pal()(1)[1], group='Sim') +
	geom_density(colour=ggthemes::tableau_color_pal()(2)[2], group='Dat', data=df_raw)

```
A predicted earlier bloom.
